{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gabriellegustilo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gabriellegustilo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    wn = nltk.WordNetLemmatizer() #you'll need to download wordnet from nltk\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "def lemmatize(in_path, out_path, file):\n",
    "    '''\n",
    "    takes a text file, strips it of punctuation, then tokenizes the data.\n",
    "    After tokenization, the stop words are removed.\n",
    "    Then the lemmas of each word is found and returned as a list.\n",
    "    '''\n",
    "    with open(in_path + '/' + file) as f:\n",
    "        rawData = f.read()\n",
    "        cleanData = remove_punct(rawData)\n",
    "        tokenized = tokenize(cleanData.lower())\n",
    "        text_no_stop = remove_stopwords(tokenized)\n",
    "        text_lemmatized = lemmatizing(text_no_stop)\n",
    "    out_file = file[:-4] + '_lemmatized.txt' #way to create lemmatized file name\n",
    "    with open(out_path + '/' + out_file, \"w\") as output:\n",
    "        for word in text_lemmatized:\n",
    "            output.write(str(word) + '\\n')\n",
    "    #in here for validation\n",
    "    #return lemmatizing(text_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentlemen of the Senate and Gentlemen of the House of Representatives:\n",
      "\n",
      "I was for some time apprehen\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/state-of-the-union-corpus-1989-2017/Adams1_1797.txt') as f:\n",
    "    rawData = f.read()\n",
    "    print(rawData[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = '/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/state-of-the-union-corpus-1989-2017'\n",
    "out_path = '/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/state-of-the-union-lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this to get the file names...probably more efficient way to do this, but ehhhh...\n",
    "from pathlib import Path\n",
    "path = Path(\"/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/state-of-the-union-corpus-1989-2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = [i for i in os.listdir(path) if i.endswith(\"txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reagan_1982.txt',\n",
       " 'Roosevelt_1902.txt',\n",
       " 'Wilson_1914.txt',\n",
       " 'Taft_1911.txt',\n",
       " 'Madison_1814.txt',\n",
       " 'Polk_1848.txt',\n",
       " 'Jackson_1836.txt',\n",
       " 'Johnson_1969.txt',\n",
       " 'Hoover_1930.txt',\n",
       " 'Pierce_1854.txt',\n",
       " 'Pierce_1855.txt',\n",
       " 'Hoover_1931.txt',\n",
       " 'Johnson_1968.txt',\n",
       " 'Madison_1815.txt',\n",
       " 'Taft_1910.txt',\n",
       " 'Buren_1837.txt',\n",
       " 'Wilson_1915.txt',\n",
       " 'Roosevelt_1903.txt',\n",
       " 'Reagan_1983.txt',\n",
       " 'Roosevelt_1901.txt',\n",
       " 'Taft_1912.txt',\n",
       " 'Wilson_1917.txt',\n",
       " 'Jackson_1835.txt',\n",
       " 'Pierce_1856.txt',\n",
       " 'Hoover_1932.txt',\n",
       " 'Adams1_1797.txt',\n",
       " 'Jackson_1834.txt',\n",
       " 'Madison_1816.txt',\n",
       " 'Wilson_1916.txt',\n",
       " 'Reagan_1984.txt',\n",
       " 'Roosevelt_1904.txt',\n",
       " 'Roosevelt_1938.txt',\n",
       " 'Fillmore_1852.txt',\n",
       " 'Madison_1812.txt',\n",
       " 'Jackson_1830.txt',\n",
       " 'Truman_1949.txt',\n",
       " 'Pierce_1853.txt',\n",
       " 'Truman_1948.txt',\n",
       " 'Jackson_1831.txt',\n",
       " 'Madison_1813.txt',\n",
       " 'Wilson_1913.txt',\n",
       " 'Roosevelt_1939.txt',\n",
       " 'Roosevelt_1905.txt',\n",
       " 'Reagan_1985.txt',\n",
       " 'Harrison_1889.txt',\n",
       " 'Reagan_1987.txt',\n",
       " 'Roosevelt_1907.txt',\n",
       " 'Adams1_1800.txt',\n",
       " 'Madison_1811.txt',\n",
       " 'Fillmore_1851.txt',\n",
       " 'Jackson_1833.txt',\n",
       " 'Kennedy_1963.txt',\n",
       " 'Kennedy_1962.txt',\n",
       " 'Jefferson_1808.txt',\n",
       " 'Jackson_1832.txt',\n",
       " 'Fillmore_1850.txt',\n",
       " 'Madison_1810.txt',\n",
       " 'Roosevelt_1906.txt',\n",
       " 'Reagan_1986.txt',\n",
       " 'Monroe_1820.txt',\n",
       " 'Grant_1876.txt',\n",
       " 'Cleveland_1896.txt',\n",
       " 'Lincoln_1863.txt',\n",
       " 'Tyler_1841.txt',\n",
       " 'Clinton_1995.txt',\n",
       " 'Obama_2015.txt',\n",
       " 'Buchanan_1860.txt',\n",
       " 'Obama_2014.txt',\n",
       " 'Trump_2017.txt',\n",
       " 'Clinton_1994.txt',\n",
       " 'Lincoln_1862.txt',\n",
       " 'Buren_1840.txt',\n",
       " 'Monroe_1821.txt',\n",
       " 'Monroe_1823.txt',\n",
       " 'Eisenhower_1959.txt',\n",
       " 'Grant_1875.txt',\n",
       " 'Cleveland_1895.txt',\n",
       " 'Tyler_1842.txt',\n",
       " 'McKinley_1898.txt',\n",
       " 'Clinton_1996.txt',\n",
       " 'Obama_2016.txt',\n",
       " 'Clinton_1997.txt',\n",
       " 'McKinley_1899.txt',\n",
       " 'Johnson_1868.txt',\n",
       " 'Tyler_1843.txt',\n",
       " 'Cleveland_1894.txt',\n",
       " 'Lincoln_1861.txt',\n",
       " 'Grant_1874.txt',\n",
       " 'Eisenhower_1958.txt',\n",
       " 'Monroe_1822.txt',\n",
       " 'Eisenhower_1960.txt',\n",
       " 'Bush1_1989.txt',\n",
       " 'Adams2_1827.txt',\n",
       " 'Grant_1870.txt',\n",
       " 'Clinton_1993.txt',\n",
       " 'Obama_2013.txt',\n",
       " 'Obama_2012.txt',\n",
       " 'Bush2_2008.txt',\n",
       " 'Grant_1871.txt',\n",
       " 'Adams2_1826.txt',\n",
       " 'Cleveland_1885.txt',\n",
       " 'Lincoln_1864.txt',\n",
       " 'Coolidge_1928.txt',\n",
       " 'Eisenhower_1961.txt',\n",
       " 'Monroe_1819.txt',\n",
       " 'Cleveland_1893.txt',\n",
       " 'Cleveland_1887.txt',\n",
       " 'Grant_1873.txt',\n",
       " 'Hayes_1877.txt',\n",
       " 'Tyler_1844.txt',\n",
       " 'Obama_2010.txt',\n",
       " 'Buchanan_1859.txt',\n",
       " 'Buchanan_1858.txt',\n",
       " 'Obama_2011.txt',\n",
       " 'McKinley_1900.txt',\n",
       " 'Grant_1872.txt',\n",
       " 'Cleveland_1886.txt',\n",
       " 'Adams2_1825.txt',\n",
       " 'Monroe_1818.txt',\n",
       " 'Monroe_1824.txt',\n",
       " 'Bush1_1992.txt',\n",
       " 'Roosevelt_1940.txt',\n",
       " 'Coolidge_1926.txt',\n",
       " 'Adams2_1828.txt',\n",
       " 'Bush2_2006.txt',\n",
       " 'Washington_1790.txt',\n",
       " 'Obama_2009.txt',\n",
       " 'Washington_1791.txt',\n",
       " 'Bush2_2007.txt',\n",
       " 'Coolidge_1927.txt',\n",
       " 'Roosevelt_1941.txt',\n",
       " 'Bush1_1991.txt',\n",
       " 'Roosevelt_1943.txt',\n",
       " 'Coolidge_1925.txt',\n",
       " 'Cleveland_1888.txt',\n",
       " 'Hayes_1878.txt',\n",
       " 'Bush2_2005.txt',\n",
       " 'Washington_1793.txt',\n",
       " 'Buchanan_1857.txt',\n",
       " 'Washington_1792.txt',\n",
       " 'Bush2_2004.txt',\n",
       " 'Hayes_1879.txt',\n",
       " 'Grant_1869.txt',\n",
       " 'Coolidge_1924.txt',\n",
       " 'Roosevelt_1942.txt',\n",
       " 'Bush1_1990.txt',\n",
       " 'Monroe_1817.txt',\n",
       " 'Harding_1922.txt',\n",
       " 'Eisenhower_1955.txt',\n",
       " 'Ford_1976.txt',\n",
       " 'Johnson_1865.txt',\n",
       " 'Washington_1796.txt',\n",
       " 'Trump_2018.txt',\n",
       " 'Bush2_2001.txt',\n",
       " 'Ford_1977.txt',\n",
       " 'Eisenhower_1954.txt',\n",
       " 'Harding_1921.txt',\n",
       " 'Eisenhower_1956.txt',\n",
       " 'Roosevelt_1945.txt',\n",
       " 'Coolidge_1923.txt',\n",
       " 'Ford_1975.txt',\n",
       " 'Bush2_2003.txt',\n",
       " 'Johnson_1866.txt',\n",
       " 'McKinley_1897.txt',\n",
       " 'Washington_1795.txt',\n",
       " 'Clinton_1999.txt',\n",
       " 'Clinton_1998.txt',\n",
       " 'Washington_1794.txt',\n",
       " 'Bush2_2002.txt',\n",
       " 'Hayes_1880.txt',\n",
       " 'Johnson_1867.txt',\n",
       " 'Roosevelt_1944.txt',\n",
       " 'Eisenhower_1957.txt',\n",
       " 'Harrison_1891.txt',\n",
       " 'Roosevelt_1937.txt',\n",
       " 'Madison_1809.txt',\n",
       " 'Truman_1946.txt',\n",
       " 'Truman_1952.txt',\n",
       " 'Jefferson_1805.txt',\n",
       " 'Nixon_1972.txt',\n",
       " 'Nixon_1973.txt',\n",
       " 'Jefferson_1804.txt',\n",
       " 'Truman_1953.txt',\n",
       " 'Truman_1947.txt',\n",
       " 'Arthur_1884.txt',\n",
       " 'Wilson_1920.txt',\n",
       " 'Roosevelt_1936.txt',\n",
       " 'Harrison_1890.txt',\n",
       " 'Harrison_1892.txt',\n",
       " 'Reagan_1988.txt',\n",
       " 'Carter_1981.txt',\n",
       " 'Roosevelt_1908.txt',\n",
       " 'Roosevelt_1934.txt',\n",
       " 'Truman_1951.txt',\n",
       " 'Jefferson_1806.txt',\n",
       " 'Nixon_1971.txt',\n",
       " 'Nixon_1970.txt',\n",
       " 'Jefferson_1807.txt',\n",
       " 'Truman_1950.txt',\n",
       " 'Jackson_1829.txt',\n",
       " 'Roosevelt_1935.txt',\n",
       " 'Carter_1980.txt',\n",
       " 'Buren_1839.txt',\n",
       " 'Taylor_1849.txt',\n",
       " 'Arthur_1883.txt',\n",
       " 'Polk_1847.txt',\n",
       " 'Jefferson_1803.txt',\n",
       " 'Johnson_1966.txt',\n",
       " 'Nixon_1974.txt',\n",
       " 'Johnson_1967.txt',\n",
       " 'Jefferson_1802.txt',\n",
       " 'Polk_1846.txt',\n",
       " 'Arthur_1882.txt',\n",
       " 'Buren_1838.txt',\n",
       " 'Carter_1978.txt',\n",
       " 'Wilson_1918.txt',\n",
       " 'Taft_1909.txt',\n",
       " 'Adams1_1799.txt',\n",
       " 'Johnson_1965.txt',\n",
       " 'Clinton_2000.txt',\n",
       " 'Hoover_1929.txt',\n",
       " 'Adams1_1798.txt',\n",
       " 'Johnson_1964.txt',\n",
       " 'Jefferson_1801.txt',\n",
       " 'Polk_1845.txt',\n",
       " 'Arthur_1881.txt',\n",
       " 'Wilson_1919.txt',\n",
       " 'Carter_1979.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now loop through all of them and write to the folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    lemmatize(in_path, out_path, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1797</td>\n",
       "      <td>Adams1</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1798</td>\n",
       "      <td>Adams1</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1799</td>\n",
       "      <td>Adams1</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1800</td>\n",
       "      <td>Adams1</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1825</td>\n",
       "      <td>Adams2</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year    name                                               text\n",
       "0           1  1797  Adams1  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...\n",
       "1           2  1798  Adams1  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...\n",
       "2           3  1799  Adams1  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...\n",
       "3           4  1800  Adams1  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...\n",
       "4           5  1825  Adams2  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "\n",
    "# Read data into papers\n",
    "speeches = pd.read_csv('/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/speeches.csv')\n",
    "\n",
    "# Print head\n",
    "speeches.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove punctuation\n",
    "speeches['text_processed'] = speeches['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "speeches['text_processed'] = speeches['text_processed'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "speeches['text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud #need to pip install this\n",
    "# Join the different processed titles together.\n",
    "\n",
    "long_string = ','.join(list(speeches['text'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(speeches['text_processed'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 10\n",
    "number_words = 30\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyLDAvis import sklearn as sklearn_lda #need to pip install this\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('../ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, '../ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO ALL OF THE ABOVE BUT WITH LEMMATIZED DATA\n",
    "\n",
    "# Importing modules\n",
    "import pandas as pd\n",
    "\n",
    "# Read data into papers\n",
    "speeches = pd.read_csv('/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/speeches.csv')\n",
    "\n",
    "# Print head\n",
    "speeches.head()\n",
    "\n",
    "import re\n",
    "# Remove punctuation\n",
    "speeches['lemmatizedText'] = speeches['lemmatizedText'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "speeches['lemmatizedText'] = speeches['lemmatizedText'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "speeches['lemmatizedText'].head()\n",
    "\n",
    "# Import the wordcloud library\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud #need to pip install this\n",
    "# Join the different processed titles together.\n",
    "\n",
    "long_string = ','.join(list(speeches['lemmatizedText'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(speeches['lemmatizedText'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adams1\n",
      "1797\n",
      "Adams1\n",
      "1798\n",
      "Adams1\n",
      "1799\n",
      "Adams1\n",
      "1800\n",
      "Adams2\n",
      "1825\n",
      "Adams2\n",
      "1826\n",
      "Adams2\n",
      "1827\n",
      "Adams2\n",
      "1828\n",
      "Arthur\n",
      "1881\n",
      "Arthur\n",
      "1882\n",
      "Arthur\n",
      "1883\n",
      "Arthur\n",
      "1884\n",
      "Buchanan\n",
      "1857\n",
      "Buchanan\n",
      "1858\n",
      "Buchanan\n",
      "1859\n",
      "Buchanan\n",
      "1860\n",
      "Buren\n",
      "1837\n",
      "Buren\n",
      "1838\n",
      "Buren\n",
      "1839\n",
      "Buren\n",
      "1840\n",
      "Bush1\n",
      "1989\n",
      "Bush1\n",
      "1990\n",
      "Bush1\n",
      "1991\n",
      "Bush1\n",
      "1992\n",
      "Bush2\n",
      "2001\n",
      "Bush2\n",
      "2002\n",
      "Bush2\n",
      "2003\n",
      "Bush2\n",
      "2004\n",
      "Bush2\n",
      "2005\n",
      "Bush2\n",
      "2006\n",
      "Bush2\n",
      "2007\n",
      "Bush2\n",
      "2008\n",
      "Carter\n",
      "1978\n",
      "Carter\n",
      "1979\n",
      "Carter\n",
      "1980\n",
      "Carter\n",
      "1981\n",
      "Cleveland\n",
      "1885\n",
      "Cleveland\n",
      "1886\n",
      "Cleveland\n",
      "1887\n",
      "Cleveland\n",
      "1888\n",
      "Cleveland\n",
      "1893\n",
      "Cleveland\n",
      "1894\n",
      "Cleveland\n",
      "1895\n",
      "Cleveland\n",
      "1896\n",
      "Clinton\n",
      "1993\n",
      "Clinton\n",
      "1994\n",
      "Clinton\n",
      "1995\n",
      "Clinton\n",
      "1996\n",
      "Clinton\n",
      "1997\n",
      "Clinton\n",
      "1998\n",
      "Clinton\n",
      "1999\n",
      "Clinton\n",
      "2000\n",
      "Coolidge\n",
      "1923\n",
      "Coolidge\n",
      "1924\n",
      "Coolidge\n",
      "1925\n",
      "Coolidge\n",
      "1926\n",
      "Coolidge\n",
      "1927\n",
      "Coolidge\n",
      "1928\n",
      "Eisenhower\n",
      "1954\n",
      "Eisenhower\n",
      "1955\n",
      "Eisenhower\n",
      "1956\n",
      "Eisenhower\n",
      "1957\n",
      "Eisenhower\n",
      "1958\n",
      "Eisenhower\n",
      "1959\n",
      "Eisenhower\n",
      "1960\n",
      "Eisenhower\n",
      "1961\n",
      "Fillmore\n",
      "1850\n",
      "Fillmore\n",
      "1851\n",
      "Fillmore\n",
      "1852\n",
      "Ford\n",
      "1975\n",
      "Ford\n",
      "1976\n",
      "Ford\n",
      "1977\n",
      "Grant\n",
      "1869\n",
      "Grant\n",
      "1870\n",
      "Grant\n",
      "1871\n",
      "Grant\n",
      "1872\n",
      "Grant\n",
      "1873\n",
      "Grant\n",
      "1874\n",
      "Grant\n",
      "1875\n",
      "Grant\n",
      "1876\n",
      "Harding\n",
      "1921\n",
      "Harding\n",
      "1922\n",
      "Harrison\n",
      "1889\n",
      "Harrison\n",
      "1890\n",
      "Harrison\n",
      "1891\n",
      "Harrison\n",
      "1892\n",
      "Hayes\n",
      "1877\n",
      "Hayes\n",
      "1878\n",
      "Hayes\n",
      "1879\n",
      "Hayes\n",
      "1880\n",
      "Hoover\n",
      "1929\n",
      "Hoover\n",
      "1930\n",
      "Hoover\n",
      "1931\n",
      "Hoover\n",
      "1932\n",
      "Jackson\n",
      "1829\n",
      "Jackson\n",
      "1830\n",
      "Jackson\n",
      "1831\n",
      "Jackson\n",
      "1832\n",
      "Jackson\n",
      "1833\n",
      "Jackson\n",
      "1834\n",
      "Jackson\n",
      "1835\n",
      "Jackson\n",
      "1836\n",
      "Jefferson\n",
      "1801\n",
      "Jefferson\n",
      "1802\n",
      "Jefferson\n",
      "1803\n",
      "Jefferson\n",
      "1804\n",
      "Jefferson\n",
      "1805\n",
      "Jefferson\n",
      "1806\n",
      "Jefferson\n",
      "1807\n",
      "Jefferson\n",
      "1808\n",
      "Johnson\n",
      "1865\n",
      "Johnson\n",
      "1866\n",
      "Johnson\n",
      "1867\n",
      "Johnson\n",
      "1868\n",
      "Johnson\n",
      "1964\n",
      "Johnson\n",
      "1965\n",
      "Johnson\n",
      "1966\n",
      "Johnson\n",
      "1967\n",
      "Johnson\n",
      "1968\n",
      "Johnson\n",
      "1969\n",
      "Kennedy\n",
      "1962\n",
      "Kennedy\n",
      "1963\n",
      "Lincoln\n",
      "1861\n",
      "Lincoln\n",
      "1862\n",
      "Lincoln\n",
      "1863\n",
      "Lincoln\n",
      "1864\n",
      "Madison\n",
      "1809\n",
      "Madison\n",
      "1810\n",
      "Madison\n",
      "1811\n",
      "Madison\n",
      "1812\n",
      "Madison\n",
      "1813\n",
      "Madison\n",
      "1814\n",
      "Madison\n",
      "1815\n",
      "Madison\n",
      "1816\n",
      "McKinley\n",
      "1897\n",
      "McKinley\n",
      "1898\n",
      "McKinley\n",
      "1899\n",
      "McKinley\n",
      "1900\n",
      "Monroe\n",
      "1817\n",
      "Monroe\n",
      "1818\n",
      "Monroe\n",
      "1819\n",
      "Monroe\n",
      "1820\n",
      "Monroe\n",
      "1821\n",
      "Monroe\n",
      "1822\n",
      "Monroe\n",
      "1823\n",
      "Monroe\n",
      "1824\n",
      "Nixon\n",
      "1970\n",
      "Nixon\n",
      "1971\n",
      "Nixon\n",
      "1972\n",
      "Nixon\n",
      "1973\n",
      "Nixon\n",
      "1974\n",
      "Obama\n",
      "2009\n",
      "Obama\n",
      "2010\n",
      "Obama\n",
      "2011\n",
      "Obama\n",
      "2012\n",
      "Obama\n",
      "2013\n",
      "Obama\n",
      "2014\n",
      "Obama\n",
      "2015\n",
      "Obama\n",
      "2016\n",
      "Pierce\n",
      "1853\n",
      "Pierce\n",
      "1854\n",
      "Pierce\n",
      "1855\n",
      "Pierce\n",
      "1856\n",
      "Polk\n",
      "1845\n",
      "Polk\n",
      "1846\n",
      "Polk\n",
      "1847\n",
      "Polk\n",
      "1848\n",
      "Reagan\n",
      "1982\n",
      "Reagan\n",
      "1983\n",
      "Reagan\n",
      "1984\n",
      "Reagan\n",
      "1985\n",
      "Reagan\n",
      "1986\n",
      "Reagan\n",
      "1987\n",
      "Reagan\n",
      "1988\n",
      "Roosevelt\n",
      "1901\n",
      "Roosevelt\n",
      "1902\n",
      "Roosevelt\n",
      "1903\n",
      "Roosevelt\n",
      "1904\n",
      "Roosevelt\n",
      "1905\n",
      "Roosevelt\n",
      "1906\n",
      "Roosevelt\n",
      "1907\n",
      "Roosevelt\n",
      "1908\n",
      "Roosevelt\n",
      "1934\n",
      "Roosevelt\n",
      "1935\n",
      "Roosevelt\n",
      "1936\n",
      "Roosevelt\n",
      "1937\n",
      "Roosevelt\n",
      "1938\n",
      "Roosevelt\n",
      "1939\n",
      "Roosevelt\n",
      "1940\n",
      "Roosevelt\n",
      "1941\n",
      "Roosevelt\n",
      "1942\n",
      "Roosevelt\n",
      "1943\n",
      "Roosevelt\n",
      "1944\n",
      "Roosevelt\n",
      "1945\n",
      "Taft\n",
      "1909\n",
      "Taft\n",
      "1910\n",
      "Taft\n",
      "1911\n",
      "Taft\n",
      "1912\n",
      "Taylor\n",
      "1849\n",
      "Truman\n",
      "1946\n",
      "Truman\n",
      "1947\n",
      "Truman\n",
      "1948\n",
      "Truman\n",
      "1949\n",
      "Truman\n",
      "1950\n",
      "Truman\n",
      "1951\n",
      "Truman\n",
      "1952\n",
      "Truman\n",
      "1953\n",
      "Trump\n",
      "2017\n",
      "Trump\n",
      "2018\n",
      "Tyler\n",
      "1841\n",
      "Tyler\n",
      "1842\n",
      "Tyler\n",
      "1843\n",
      "Tyler\n",
      "1844\n",
      "Washington\n",
      "1790\n",
      "Washington\n",
      "1791\n",
      "Washington\n",
      "1792\n",
      "Washington\n",
      "1793\n",
      "Washington\n",
      "1794\n",
      "Washington\n",
      "1795\n",
      "Washington\n",
      "1796\n",
      "Wilson\n",
      "1913\n",
      "Wilson\n",
      "1914\n",
      "Wilson\n",
      "1915\n",
      "Wilson\n",
      "1916\n",
      "Wilson\n",
      "1917\n",
      "Wilson\n",
      "1918\n",
      "Wilson\n",
      "1919\n",
      "Wilson\n",
      "1920\n"
     ]
    }
   ],
   "source": [
    "# we can see that the lemmatized data doesn't do anything different, so we stop here\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Import the wordcloud library\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud #need to pip install this\n",
    "# Join the different processed titles together.\n",
    "\n",
    "# Importing modules\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text \n",
    "import re\n",
    "\n",
    "# Read data into papers\n",
    "speeches = pd.read_csv('/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/speeches.csv')\n",
    "\n",
    "# Print head\n",
    "speeches.head()\n",
    "\n",
    "my_stop_words = [\"government\", \"congress\", \"faith\", \"democracy\", \"year\", \"annual\", \"people\", \"utter\"]\n",
    "stop_words = frozenset(text.ENGLISH_STOP_WORDS.union(my_stop_words))\n",
    "\n",
    "# Remove punctuation\n",
    "speeches['text_processed'] = speeches['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "speeches['text_processed'] = speeches['text_processed'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "speeches['text_processed'].head()\n",
    "\n",
    "def analyze_each_speech():\n",
    "    for i in range(len(speeches['text_processed'])):\n",
    "        print(speeches['name'][i])\n",
    "        print(speeches['year'][i])\n",
    "        # Create a WordCloud object\n",
    "        wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "        # Generate a word cloud\n",
    "        wordcloud.generate(speeches['text_processed'][i])\n",
    "    \n",
    "\n",
    "        # Visualize the word cloud\n",
    "        wordcloud.to_image()\n",
    "        filename = \"./wordcloud_img_stopwords/%s_%s_wordcloud.png\" %(speeches['name'][i], speeches['year'][i])\n",
    "        wordcloud.to_file(filename)\n",
    "\n",
    "analyze_each_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adams1\n",
      "1797\n",
      "Adams1\n",
      "1798\n",
      "Adams1\n",
      "1799\n",
      "Adams1\n",
      "1800\n",
      "Adams2\n",
      "1825\n",
      "Adams2\n",
      "1826\n",
      "Adams2\n",
      "1827\n",
      "Adams2\n",
      "1828\n",
      "Arthur\n",
      "1881\n",
      "Arthur\n",
      "1882\n",
      "Arthur\n",
      "1883\n",
      "Arthur\n",
      "1884\n",
      "Buchanan\n",
      "1857\n",
      "Buchanan\n",
      "1858\n",
      "Buchanan\n",
      "1859\n",
      "Buchanan\n",
      "1860\n",
      "Buren\n",
      "1837\n",
      "Buren\n",
      "1838\n",
      "Buren\n",
      "1839\n",
      "Buren\n",
      "1840\n",
      "Bush1\n",
      "1989\n",
      "Bush1\n",
      "1990\n",
      "Bush1\n",
      "1991\n",
      "Bush1\n",
      "1992\n",
      "Bush2\n",
      "2001\n",
      "Bush2\n",
      "2002\n",
      "Bush2\n",
      "2003\n",
      "Bush2\n",
      "2004\n",
      "Bush2\n",
      "2005\n",
      "Bush2\n",
      "2006\n",
      "Bush2\n",
      "2007\n",
      "Bush2\n",
      "2008\n",
      "Carter\n",
      "1978\n",
      "Carter\n",
      "1979\n",
      "Carter\n",
      "1980\n",
      "Carter\n",
      "1981\n",
      "Cleveland\n",
      "1885\n",
      "Cleveland\n",
      "1886\n",
      "Cleveland\n",
      "1887\n",
      "Cleveland\n",
      "1888\n",
      "Cleveland\n",
      "1893\n",
      "Cleveland\n",
      "1894\n",
      "Cleveland\n",
      "1895\n",
      "Cleveland\n",
      "1896\n",
      "Clinton\n",
      "1993\n",
      "Clinton\n",
      "1994\n",
      "Clinton\n",
      "1995\n",
      "Clinton\n",
      "1996\n",
      "Clinton\n",
      "1997\n",
      "Clinton\n",
      "1998\n",
      "Clinton\n",
      "1999\n",
      "Clinton\n",
      "2000\n",
      "Coolidge\n",
      "1923\n",
      "Coolidge\n",
      "1924\n",
      "Coolidge\n",
      "1925\n",
      "Coolidge\n",
      "1926\n",
      "Coolidge\n",
      "1927\n",
      "Coolidge\n",
      "1928\n",
      "Eisenhower\n",
      "1954\n",
      "Eisenhower\n",
      "1955\n",
      "Eisenhower\n",
      "1956\n",
      "Eisenhower\n",
      "1957\n",
      "Eisenhower\n",
      "1958\n",
      "Eisenhower\n",
      "1959\n",
      "Eisenhower\n",
      "1960\n",
      "Eisenhower\n",
      "1961\n",
      "Fillmore\n",
      "1850\n",
      "Fillmore\n",
      "1851\n",
      "Fillmore\n",
      "1852\n",
      "Ford\n",
      "1975\n",
      "Ford\n",
      "1976\n",
      "Ford\n",
      "1977\n",
      "Grant\n",
      "1869\n",
      "Grant\n",
      "1870\n",
      "Grant\n",
      "1871\n",
      "Grant\n",
      "1872\n",
      "Grant\n",
      "1873\n",
      "Grant\n",
      "1874\n",
      "Grant\n",
      "1875\n",
      "Grant\n",
      "1876\n",
      "Harding\n",
      "1921\n",
      "Harding\n",
      "1922\n",
      "Harrison\n",
      "1889\n",
      "Harrison\n",
      "1890\n",
      "Harrison\n",
      "1891\n",
      "Harrison\n",
      "1892\n",
      "Hayes\n",
      "1877\n",
      "Hayes\n",
      "1878\n",
      "Hayes\n",
      "1879\n",
      "Hayes\n",
      "1880\n",
      "Hoover\n",
      "1929\n",
      "Hoover\n",
      "1930\n",
      "Hoover\n",
      "1931\n",
      "Hoover\n",
      "1932\n",
      "Jackson\n",
      "1829\n",
      "Jackson\n",
      "1830\n",
      "Jackson\n",
      "1831\n",
      "Jackson\n",
      "1832\n",
      "Jackson\n",
      "1833\n",
      "Jackson\n",
      "1834\n",
      "Jackson\n",
      "1835\n",
      "Jackson\n",
      "1836\n",
      "Jefferson\n",
      "1801\n",
      "Jefferson\n",
      "1802\n",
      "Jefferson\n",
      "1803\n",
      "Jefferson\n",
      "1804\n",
      "Jefferson\n",
      "1805\n",
      "Jefferson\n",
      "1806\n",
      "Jefferson\n",
      "1807\n",
      "Jefferson\n",
      "1808\n",
      "Johnson\n",
      "1865\n",
      "Johnson\n",
      "1866\n",
      "Johnson\n",
      "1867\n",
      "Johnson\n",
      "1868\n",
      "Johnson\n",
      "1964\n",
      "Johnson\n",
      "1965\n",
      "Johnson\n",
      "1966\n",
      "Johnson\n",
      "1967\n",
      "Johnson\n",
      "1968\n",
      "Johnson\n",
      "1969\n",
      "Kennedy\n",
      "1962\n",
      "Kennedy\n",
      "1963\n",
      "Lincoln\n",
      "1861\n",
      "Lincoln\n",
      "1862\n",
      "Lincoln\n",
      "1863\n",
      "Lincoln\n",
      "1864\n",
      "Madison\n",
      "1809\n",
      "Madison\n",
      "1810\n",
      "Madison\n",
      "1811\n",
      "Madison\n",
      "1812\n",
      "Madison\n",
      "1813\n",
      "Madison\n",
      "1814\n",
      "Madison\n",
      "1815\n",
      "Madison\n",
      "1816\n",
      "McKinley\n",
      "1897\n",
      "McKinley\n",
      "1898\n",
      "McKinley\n",
      "1899\n",
      "McKinley\n",
      "1900\n",
      "Monroe\n",
      "1817\n",
      "Monroe\n",
      "1818\n",
      "Monroe\n",
      "1819\n",
      "Monroe\n",
      "1820\n",
      "Monroe\n",
      "1821\n",
      "Monroe\n",
      "1822\n",
      "Monroe\n",
      "1823\n",
      "Monroe\n",
      "1824\n",
      "Nixon\n",
      "1970\n",
      "Nixon\n",
      "1971\n",
      "Nixon\n",
      "1972\n",
      "Nixon\n",
      "1973\n",
      "Nixon\n",
      "1974\n",
      "Obama\n",
      "2009\n",
      "Obama\n",
      "2010\n",
      "Obama\n",
      "2011\n",
      "Obama\n",
      "2012\n",
      "Obama\n",
      "2013\n",
      "Obama\n",
      "2014\n",
      "Obama\n",
      "2015\n",
      "Obama\n",
      "2016\n",
      "Pierce\n",
      "1853\n",
      "Pierce\n",
      "1854\n",
      "Pierce\n",
      "1855\n",
      "Pierce\n",
      "1856\n",
      "Polk\n",
      "1845\n",
      "Polk\n",
      "1846\n",
      "Polk\n",
      "1847\n",
      "Polk\n",
      "1848\n",
      "Reagan\n",
      "1982\n",
      "Reagan\n",
      "1983\n",
      "Reagan\n",
      "1984\n",
      "Reagan\n",
      "1985\n",
      "Reagan\n",
      "1986\n",
      "Reagan\n",
      "1987\n",
      "Reagan\n",
      "1988\n",
      "Roosevelt\n",
      "1901\n",
      "Roosevelt\n",
      "1902\n",
      "Roosevelt\n",
      "1903\n",
      "Roosevelt\n",
      "1904\n",
      "Roosevelt\n",
      "1905\n",
      "Roosevelt\n",
      "1906\n",
      "Roosevelt\n",
      "1907\n",
      "Roosevelt\n",
      "1908\n",
      "Roosevelt\n",
      "1934\n",
      "Roosevelt\n",
      "1935\n",
      "Roosevelt\n",
      "1936\n",
      "Roosevelt\n",
      "1937\n",
      "Roosevelt\n",
      "1938\n",
      "Roosevelt\n",
      "1939\n",
      "Roosevelt\n",
      "1940\n",
      "Roosevelt\n",
      "1941\n",
      "Roosevelt\n",
      "1942\n",
      "Roosevelt\n",
      "1943\n",
      "Roosevelt\n",
      "1944\n",
      "Roosevelt\n",
      "1945\n",
      "Taft\n",
      "1909\n",
      "Taft\n",
      "1910\n",
      "Taft\n",
      "1911\n",
      "Taft\n",
      "1912\n",
      "Taylor\n",
      "1849\n",
      "Truman\n",
      "1946\n",
      "Truman\n",
      "1947\n",
      "Truman\n",
      "1948\n",
      "Truman\n",
      "1949\n",
      "Truman\n",
      "1950\n",
      "Truman\n",
      "1951\n",
      "Truman\n",
      "1952\n",
      "Truman\n",
      "1953\n",
      "Trump\n",
      "2017\n",
      "Trump\n",
      "2018\n",
      "Tyler\n",
      "1841\n",
      "Tyler\n",
      "1842\n",
      "Tyler\n",
      "1843\n",
      "Tyler\n",
      "1844\n",
      "Washington\n",
      "1790\n",
      "Washington\n",
      "1791\n",
      "Washington\n",
      "1792\n",
      "Washington\n",
      "1793\n",
      "Washington\n",
      "1794\n",
      "Washington\n",
      "1795\n",
      "Washington\n",
      "1796\n",
      "Wilson\n",
      "1913\n",
      "Wilson\n",
      "1914\n",
      "Wilson\n",
      "1915\n",
      "Wilson\n",
      "1916\n",
      "Wilson\n",
      "1917\n",
      "Wilson\n",
      "1918\n",
      "Wilson\n",
      "1919\n",
      "Wilson\n",
      "1920\n"
     ]
    }
   ],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import re\n",
    "# Importing modules\n",
    "import pandas as pd\n",
    "\n",
    "# Read data into papers\n",
    "speeches = pd.read_csv('/Users/gabriellegustilo/Dev/personal/machine_learning/final_project/speeches.csv')\n",
    "\n",
    "# Print head\n",
    "speeches.head()\n",
    "\n",
    "# Remove punctuation\n",
    "speeches['text_processed'] = speeches['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "speeches['text_processed'] = speeches['text_processed'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "speeches['text_processed'].head()\n",
    "\n",
    "# Tweak the two parameters below\n",
    "number_topics = 10\n",
    "number_words = 5\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "\n",
    "my_stop_words = [\"government\", \"congress\", \"faith\", \"democracy\", \"year\", \"annual\", \"people\", \"utter\"]\n",
    "\n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words, filename):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    with open(filename, 'w') as f:\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            f.write(''.join((\"\\n\", \" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))))  \n",
    "\n",
    "\n",
    "# Helper function\n",
    "def save_topics_from_most_common_words(count_data, count_vectorizer, name):    \n",
    "    lda.fit(count_data)\n",
    "\n",
    "    # Print the topics found by the LDA model\n",
    "\n",
    "    print_topics(lda, count_vectorizer, number_words, name)\n",
    "    \n",
    "\n",
    "def get_topics_for_each_speech():\n",
    "    for i in range(len(speeches['text_processed'])):\n",
    "        print(speeches['name'][i])\n",
    "        print(speeches['year'][i])\n",
    "        #print(speeches['text_processed'][i])\n",
    "        \n",
    "        stop_words = frozenset(text.ENGLISH_STOP_WORDS.union(my_stop_words))\n",
    "        \n",
    "        # Initialise the count vectorizer with the English stop words\n",
    "        count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "        # Fit and transform the processed titles\n",
    "        count_data = count_vectorizer.fit_transform(speeches['text_processed'][i].splitlines())\n",
    "        \n",
    "        # get filename\n",
    "        filename = \"./topics/%s_%s_topics.txt\" %(speeches['name'][i], speeches['year'][i])\n",
    "\n",
    "        # Visualise the 10 most common words\n",
    "        save_topics_from_most_common_words(count_data, count_vectorizer, filename)\n",
    "\n",
    "\n",
    "get_topics_for_each_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
